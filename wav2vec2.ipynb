{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3648,
     "status": "ok",
     "timestamp": 1745055957100,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "COywOa-waun0",
    "outputId": "6b30878c-f3c7-4e97-c334-9817e7fddf0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechbrain in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
      "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.2.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.4.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.14.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->speechbrain) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain) (0.18.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install speechbrain torchaudio transformers peft datasets psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "6915a3d83fdc48ec94a5eebf9610d9e2",
      "414a33454d6e476e9c723049bf352fbf",
      "ecd290cd8f654781ac7155c871780ff7",
      "5b13e0c675384e01b3d10ac99c30c3e7",
      "7c06dd72991b4c7b95e865025b9a9d0a",
      "e3cb35424ecb406ca2b335e0d84c22d5",
      "96f573177715493b8af24605dcac656b",
      "8a6ab37f30864d1980d894c26b1fd214",
      "5b6b1f0c7fe3438eb96111f06751f219",
      "07599a3216ed439db21bd171f6c584ac",
      "6d7044f841db4d7e93ca612515086c32",
      "fb4a2506a8174091af5b5f45b461a7ca",
      "e0509ee9a5f54b078d739354d6c1450f",
      "95aa0a42a8ce4581bfcf403ed48567b3",
      "6916f530f23e4dccb4acad23f412d95a",
      "a73e15a435bc4483bb5051451da4f1a5",
      "f769e30126474b0cb71608ff68d123b6",
      "6cb88910158b46439e0f2f8a92979a35",
      "8946152ce758423faa81172d8cd89eaa",
      "6e9a01c0d282452e8c70d15050a292cb",
      "38fcf1c7aeea4c899a5bc3143b3916e4",
      "353830735b4e44d58359b151e3e3c07b"
     ]
    },
    "executionInfo": {
     "elapsed": 44104,
     "status": "ok",
     "timestamp": 1745057189062,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "pCCbZQ3Td9V9",
    "outputId": "8240679a-dfa0-4b35-cd88-53c5ea953bfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6915a3d83fdc48ec94a5eebf9610d9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a2506a8174091af5b5f45b461a7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully uploaded to Hugging Face Hub: DhulipallaGopiChandu/wav2vec2-base-960h\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfApi\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Login using your Hugging Face token\n",
    "login(token=\"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\")\n",
    "\n",
    "\n",
    "# Load base model\n",
    "model_id = \"facebook/wav2vec2-base-960h\"\n",
    "model_name_hf = \"DhulipallaGopiChandu/wav2vec2-base-960h\"\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "# Push to hub\n",
    "model.push_to_hub(model_name_hf)\n",
    "processor.push_to_hub(model_name_hf)\n",
    "\n",
    "print(f\"Model successfully uploaded to Hugging Face Hub: {model_name_hf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSb5KpYSd9ZZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13620,
     "status": "ok",
     "timestamp": 1745058090973,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "7uVxEMQba5KL",
    "outputId": "1415cc1f-044b-4970-b23c-2330bddc29e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/content/tmp/hyperparams.yaml'\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'TalTechNLP/voxlingua107-epaca-tdnn' if not cached\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in tmp.\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/content/tmp/embedding_model.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/tmp/embedding_model.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/content/tmp/classifier.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/tmp/classifier.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/content/tmp/label_encoder.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /content/tmp/label_encoder.ckpt\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, classifier, label_encoder\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/tmp/embedding_model.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/tmp/classifier.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /content/tmp/label_encoder.ckpt\n",
      "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /content/tmp/label_encoder.ckpt\n",
      "WARNING:speechbrain.dataio.encoder:CategoricalEncoder.expect_len was never called: assuming category count of 107 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language prediction score vector: tensor([[0.2036, 0.4205, 0.4189, 0.4430, 0.4253, 0.3307, 0.3651, 0.3297, 0.3800,\n",
      "         0.4517, 0.3850, 0.4028, 0.4339, 0.4476, 0.6935, 0.4069, 0.4663, 0.4284,\n",
      "         0.4065, 0.4165, 0.7044, 0.6059, 0.5241, 0.3350, 0.3693, 0.4222, 0.5692,\n",
      "         0.3966, 0.3593, 0.4879, 0.3984, 0.4530, 0.4350, 0.5091, 0.5765, 0.4692,\n",
      "         0.4774, 0.4723, 0.4451, 0.3418, 0.5385, 0.4734, 0.4498, 0.4621, 0.4069,\n",
      "         0.4299, 0.4723, 0.3745, 0.3644, 0.4633, 0.5024, 0.4404, 0.5255, 0.3712,\n",
      "         0.4351, 0.3835, 0.4044, 0.4357, 0.3829, 0.5408, 0.4475, 0.5493, 0.3823,\n",
      "         0.4783, 0.5255, 0.4427, 0.4595, 0.4276, 0.4455, 0.4445, 0.3932, 0.3282,\n",
      "         0.4089, 0.4026, 0.4154, 0.4511, 0.4402, 0.3843, 0.3928, 0.5270, 0.5304,\n",
      "         0.4969, 0.4924, 0.4663, 0.4525, 0.4962, 0.3995, 0.3916, 0.5029, 0.4550,\n",
      "         0.4364, 0.4374, 0.5449, 0.3133, 0.4373, 0.3732, 0.5400, 0.3557, 0.4078,\n",
      "         0.3705, 0.4712, 0.3552, 0.4260, 0.6307, 0.3756, 0.4526, 0.3739]])\n",
      "Top language ISO code: ['en']\n",
      "All labels: <speechbrain.dataio.encoder.CategoricalEncoder object at 0x7ab026949bd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/content/tmp/hyperparams.yaml'\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'TalTechNLP/voxlingua107-epaca-tdnn' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 1, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in tmp.\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/content/tmp/embedding_model.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/tmp/embedding_model.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/content/tmp/classifier.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/tmp/classifier.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/content/tmp/label_encoder.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /content/tmp/label_encoder.ckpt\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, classifier, label_encoder\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/tmp/embedding_model.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/tmp/classifier.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /content/tmp/label_encoder.ckpt\n",
      "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /content/tmp/label_encoder.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language prediction score vector: tensor([[0.2036, 0.4205, 0.4189, 0.4430, 0.4253, 0.3307, 0.3651, 0.3297, 0.3800,\n",
      "         0.4517, 0.3850, 0.4028, 0.4339, 0.4476, 0.6935, 0.4069, 0.4663, 0.4284,\n",
      "         0.4065, 0.4165, 0.7044, 0.6059, 0.5241, 0.3350, 0.3693, 0.4222, 0.5692,\n",
      "         0.3966, 0.3593, 0.4879, 0.3984, 0.4530, 0.4350, 0.5091, 0.5765, 0.4692,\n",
      "         0.4774, 0.4723, 0.4451, 0.3418, 0.5385, 0.4734, 0.4498, 0.4621, 0.4069,\n",
      "         0.4299, 0.4723, 0.3745, 0.3644, 0.4633, 0.5024, 0.4404, 0.5255, 0.3712,\n",
      "         0.4351, 0.3835, 0.4044, 0.4357, 0.3829, 0.5408, 0.4475, 0.5493, 0.3823,\n",
      "         0.4783, 0.5255, 0.4427, 0.4595, 0.4276, 0.4455, 0.4445, 0.3932, 0.3282,\n",
      "         0.4089, 0.4026, 0.4154, 0.4511, 0.4402, 0.3843, 0.3928, 0.5270, 0.5304,\n",
      "         0.4969, 0.4924, 0.4663, 0.4525, 0.4962, 0.3995, 0.3916, 0.5029, 0.4550,\n",
      "         0.4364, 0.4374, 0.5449, 0.3133, 0.4373, 0.3732, 0.5400, 0.3557, 0.4078,\n",
      "         0.3705, 0.4712, 0.3552, 0.4260, 0.6307, 0.3756, 0.4526, 0.3739]])\n",
      "Top language ISO code: ['en']\n",
      "All labels: <speechbrain.dataio.encoder.CategoricalEncoder object at 0x7ab026ac3690>\n",
      "Embedding shape: torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from speechbrain.inference.classifiers import EncoderClassifier\n",
    "import torchaudio\n",
    "\n",
    "language_id = EncoderClassifier.from_hparams(source=\"TalTechNLP/voxlingua107-epaca-tdnn\", savedir=\"tmp\")\n",
    "\n",
    "\n",
    "signal = language_id.load_audio(\"/content/sample.mp3.m4a\")\n",
    "prediction = language_id.classify_batch(signal)\n",
    "\n",
    "print(\"Language prediction score vector:\", prediction[0])\n",
    "print(\"Top language ISO code:\", prediction[3])\n",
    "print(\"All labels:\", language_id.hparams.label_encoder)\n",
    "\n",
    "emb = language_id.encode_batch(signal)\n",
    "print(\"Embedding shape:\", emb.shape)\n",
    "\n",
    "\n",
    "\n",
    "from speechbrain.inference.classifiers import EncoderClassifier\n",
    "import torchaudio\n",
    "\n",
    "language_id = EncoderClassifier.from_hparams(source=\"TalTechNLP/voxlingua107-epaca-tdnn\", savedir=\"tmp\")\n",
    "\n",
    "\n",
    "signal = language_id.load_audio(\"/content/sample.mp3.m4a\")\n",
    "prediction = language_id.classify_batch(signal)\n",
    "\n",
    "print(\"Language prediction score vector:\", prediction[0])\n",
    "print(\"Top language ISO code:\", prediction[3])\n",
    "print(\"All labels:\", language_id.hparams.label_encoder)\n",
    "\n",
    "emb = language_id.encode_batch(signal)\n",
    "print(\"Embedding shape:\", emb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHWjJtS6d7Yx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7076,
     "status": "ok",
     "timestamp": 1745058115225,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "8wNhiYsrbolG",
    "outputId": "57b47c02-f5e1-4cd0-85d6-b490ed96736a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 2: Whisper Transcription\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Whisper - test from dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "audio_sample = ds[0][\"audio\"]\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "input_features = processor(audio_sample[\"array\"], sampling_rate=audio_sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "predicted_ids = model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(\"Transcription:\", transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2149,
     "status": "ok",
     "timestamp": 1745058613710,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "EK0HogHjbv03",
    "outputId": "f25aa552-9c3f-40ed-d435-60776df2acff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription from test.mp3:  you\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Whisper - transcribe from MP3\n",
    "audio_sample, sample_rate = torchaudio.load(\"/content/Recording.m4a\")\n",
    "resampled = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(audio_sample)\n",
    "\n",
    "input_features = processor(resampled.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "predicted_ids = model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(\"Transcription from test.mp3:\", transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11790,
     "status": "ok",
     "timestamp": 1745058652150,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "uEbnhunrb3-W",
    "outputId": "8a5b4b70-620a-4ff8-e4b5-f7a117112136"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Model Transcription: MAY BEOPLE ARE LYING IN THE ROAD SO ART PARTUALA TO BIT HE PEOPLE SO WE HOW TO MAKE THE PEOPLE SOME OTHER ADDRESS WE WILL DANSOM OF THESE PEOPLE DO THAT EXIPANT POLO CITY SO HAPPY WE ARE LESZENING THESE PEDIO AND BOARDCAST SO STOP RECARTING THE PEOPLE WEARE SITTING ASILENTLY IN THE ROM AS TO BE PRUVILOUS TO THE MORAL OF THEIR EXECUTE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 3: LoRA Wav2Vec2 with Upload\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load base model\n",
    "model_id = \"facebook/wav2vec2-base-960h\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "# Configure LoRA\n",
    "target_modules = []\n",
    "for i in range(12):\n",
    "    target_modules.extend([\n",
    "        f\"wav2vec2.encoder.layers.{i}.attention.k_proj\",\n",
    "        f\"wav2vec2.encoder.layers.{i}.attention.v_proj\",\n",
    "        f\"wav2vec2.encoder.layers.{i}.attention.q_proj\",\n",
    "        f\"wav2vec2.encoder.layers.{i}.attention.out_proj\",\n",
    "        f\"wav2vec2.encoder.layers.{i}.feed_forward.intermediate_dense\",\n",
    "        f\"wav2vec2.encoder.layers.{i}.feed_forward.output_dense\",\n",
    "    ])\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Test with audio\n",
    "waveform, sample_rate = torchaudio.load(\"/content/sample.mp3.m4a\")\n",
    "waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0)\n",
    "\n",
    "waveform = waveform.squeeze(0)\n",
    "input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = lora_model(input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(\"LoRA Model Transcription:\", transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "b661bdcd698c4ba09c22e0d324dc01a9",
      "dbf7770c75624822a299b0a6efee76eb",
      "dc1e618c31044614978e0f1d00633a7d",
      "74e084e002df4004a163f713aeff8267",
      "f35749b1d9134504ae35ad3d5013b384",
      "2fda1ef9c4d94071a503691728bbc97b",
      "95b05d0b4507488a980caeebe410ca8c",
      "ffaab0a9fcfa4c4284ad9dbb9b4a21ee",
      "e0a6562b50ca41688c2ada1f9f85b4be",
      "d2224ae68db84cc1a06c668c0a2e80f6",
      "ec2c7217083d491d8afab4b50d3e4ffa",
      "0a2592d358584e02a7225571f00ea935",
      "87a67343edfc4ac5ac6583cf2eb95bee",
      "010610d3cce14ba09ce2782be80fbca0",
      "a671be2ac523495db49826f588720087",
      "84b22730e7754bc7a26ca387c374e809",
      "2cf3b563bf6e4f089a05cce4c83721ce",
      "a2fb5837af274bce8b8383a87fa4c528",
      "d6897345c8dd4357af23d6f826a0bc23",
      "3759ebab065e41f7a0901bfce4fbcc64",
      "540b0e775c264b89ab577b133f9d499c",
      "892616ddf5cb414fa48c8cb894213212"
     ]
    },
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1745058744936,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "bH1q0BbDb4B0",
    "outputId": "453a1d23-d03a-43f9-94fb-b5f2812e4aeb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b661bdcd698c4ba09c22e0d324dc01a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2592d358584e02a7225571f00ea935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model successfully uploaded to Hugging Face Hub: https://huggingface.co/DhulipallaGopiChandu/wav2vec2-lora-quantized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 4: Upload LoRA Quantized Model to Hugging Face Hub\n",
    "\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Step 1: Login to Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Step 2: Set your model name (include your username)\n",
    "model_name = \"DhulipallaGopiChandu/wav2vec2-lora-quantized\"  #  Replace with your actual model name\n",
    "\n",
    "# Step 3: Upload the LoRA model and processor\n",
    "#  Make sure lora_model is already defined and trained somewhere before this step!\n",
    "lora_model.push_to_hub(model_name)\n",
    "processor.push_to_hub(model_name)\n",
    "\n",
    "# Step 4: Confirmation message\n",
    "print(f\" Model successfully uploaded to Hugging Face Hub: https://huggingface.co/{model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9735,
     "status": "ok",
     "timestamp": 1745058760169,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "LzQzEFBCcBnu",
    "outputId": "096abd80-2391-4da4-e741-59d86d8dc7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial RAM usage: 3073.04 MB\n",
      "Inference Time: 9.6858 seconds\n",
      "Post-inference RAM usage: 3078.45 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 5: Inference Time + RAM Usage\n",
    "\n",
    "import time, psutil, os\n",
    "\n",
    "def measure_inference_time(model, input_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        _ = model(input_tensor)\n",
    "        end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 ** 2)  # in MB\n",
    "\n",
    "print(f\"Initial RAM usage: {get_memory_usage():.2f} MB\")\n",
    "inference_time = measure_inference_time(lora_model, input_values)\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Post-inference RAM usage: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "74480ff31db34b6da7b9f3c1655ac553",
      "f86968b9312a46aba6e301227fdcffb9",
      "b6ec62f85eab4a3cbe9be3a0231557e3",
      "a42622ff7eb04dd6864ccde5a148b1d8",
      "3c9a4ee0c9334628b9ee3f240d9d814d",
      "2bd25a4ec2244c1b9611b40e224755dd",
      "4e620fe2609d46be9d1d19756e0fe1d4",
      "aded6538810f44f79abe44d32525dd19",
      "ce29c256fa2a4907b62e21b89503c875",
      "e901098810ce4404846e817a3acaf0b7",
      "2f968bb3960d4cc6838d9b1efb51544e",
      "5b9d3378fb6d4693b377cbe152f7cfff",
      "936faa126f974791b9ebb4ae8119fc5f",
      "162dcb48daeb4104bb096b9a57d32776",
      "7fadadeac032486e8fd96fbcebc732f1",
      "3d10f94d32324f73b096cc864a8f9640",
      "874869ad34024ad5bc9256d405b0e905",
      "e23606c815114271a7142fcabdf615e0",
      "6edac237bb1742478d3a8473dbae1d86",
      "b89f18d29cfe46aead63dbbf08c91d92",
      "5794cd23d7d04d129173594c4b5a4c12",
      "9bee943f527246fbb5d66685f261f62b"
     ]
    },
    "executionInfo": {
     "elapsed": 2672,
     "status": "ok",
     "timestamp": 1745061421685,
     "user": {
      "displayName": "Dhulipalla Gopi Chandu",
      "userId": "18384514182688660712"
     },
     "user_tz": -330
    },
    "id": "_rAPF7IMcN4H",
    "outputId": "98a043f4-92a4-4c06-d927-96e0934b65f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74480ff31db34b6da7b9f3c1655ac553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9d3378fb6d4693b377cbe152f7cfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully uploaded to Hugging Face Hub: https://huggingface.co/DhulipallaGopiChandu/wav2vec2-lora-quantized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "# Step 2: Define the model name with your username\n",
    "model_name = \"DhulipallaGopiChandu/wav2vec2-lora-quantized\"  # Replace with your desired model name\n",
    "\n",
    "# Step 3: Push LoRA model and processor to Hugging Face Hub\n",
    "lora_model.push_to_hub(model_name)\n",
    "processor.push_to_hub(model_name)\n",
    "\n",
    "# Step 4: Confirmation message\n",
    "print(f\"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/{model_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPl9sySUKNmW26SuaB3/LD1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
